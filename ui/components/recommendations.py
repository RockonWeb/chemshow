"""
–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
AI-powered –ø–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from typing import Dict, Any, List, Optional, Tuple, Union
import logging
import sqlite3
import re
import math
import os
import hashlib
import time
from collections import defaultdict
from functools import lru_cache, wraps
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã —Å –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
def safe_import():
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    imports = {}
    
    # –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç
    try:
        from .utils import get_display_name, safe_get_value, format_mass
        imports['utils'] = True
    except ImportError:
        try:
            from utils import get_display_name, safe_get_value, format_mass
            imports['utils'] = True
        except ImportError:
            logger.warning("–£—Ç–∏–ª–∏—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫–∏")
            # –ó–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π
            def get_display_name(entity, max_words=None):
                return entity.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            def safe_get_value(entity, key, default='‚Äî'):
                return entity.get(key, default)
            def format_mass(mass):
                return f"{mass:.2f}" if mass else "‚Äî"
            imports['utils'] = False
    
    # –ò–º–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
    try:
        from ..config.settings import DATABASE_PATHS
        imports['settings'] = True
    except ImportError:
        try:
            from config.settings import DATABASE_PATHS
            imports['settings'] = True
        except ImportError:
            logger.warning("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            DATABASE_PATHS = {
                "metabolites": "/workspace/data/metabolites.db",
                "enzymes": "/workspace/data/enzymes.db", 
                "proteins": "/workspace/data/proteins.db",
                "carbohydrates": "/workspace/data/carbohydrates.db",
                "lipids": "/workspace/data/lipids.db"
            }
            imports['settings'] = False
    
    # –ò–º–ø–æ—Ä—Ç RDKit
    try:
        from rdkit import Chem
        from rdkit.Chem import AllChem, DataStructs, Descriptors
        from rdkit.Chem.Fingerprints import FingerprintMols
        imports['rdkit'] = True
        logger.info("RDKit –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
    except ImportError:
        logger.warning("RDKit –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –±–∞–∑–æ–≤—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã")
        imports['rdkit'] = False
    
    return imports

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
IMPORTS = safe_import()
RDKIT_AVAILABLE = IMPORTS['rdkit']

if RDKIT_AVAILABLE:
    from rdkit import Chem
    from rdkit.Chem import AllChem, DataStructs, Descriptors


def timing_decorator(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        logger.debug(f"{func.__name__} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –∑–∞ {end_time - start_time:.3f}—Å")
        return result
    return wrapper


def cache_key(*args, **kwargs):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
    key_string = str(args) + str(sorted(kwargs.items()))
    return hashlib.md5(key_string.encode()).hexdigest()


class AdvancedCache:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å TTL –∏ —Ä–∞–∑–º–µ—Ä–æ–º"""
    
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl = ttl
    
    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞"""
        if key in self.cache:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º TTL
            if time.time() - self.access_times[key] < self.ttl:
                return self.cache[key]
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∑–∞–ø–∏—Å—å
                del self.cache[key]
                del self.access_times[key]
        return None
    
    def set(self, key: str, value: Any):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à"""
        # –û—á–∏—â–∞–µ–º –∫—ç—à –µ—Å–ª–∏ –æ–Ω –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω
        if len(self.cache) >= self.max_size:
            self._cleanup_cache()
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def _cleanup_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞"""
        current_time = time.time()
        expired_keys = [
            key for key, access_time in self.access_times.items()
            if current_time - access_time > self.ttl
        ]
        
        for key in expired_keys:
            del self.cache[key]
            del self.access_times[key]
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω, —É–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ
        if len(self.cache) >= self.max_size:
            sorted_keys = sorted(
                self.access_times.keys(),
                key=lambda k: self.access_times[k]
            )
            keys_to_remove = sorted_keys[:len(sorted_keys)//4]  # –£–¥–∞–ª—è–µ–º 25%
            for key in keys_to_remove:
                del self.cache[key]
                del self.access_times[key]


class RecommendationsEngine:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –∏ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""

    def __init__(self):
        self.compound_cache = AdvancedCache(max_size=2000, ttl=1800)  # 30 –º–∏–Ω—É—Ç
        self.similarity_cache = AdvancedCache(max_size=5000, ttl=3600)  # 1 —á–∞—Å
        self.fingerprint_cache = AdvancedCache(max_size=1000, ttl=7200)  # 2 —á–∞—Å–∞
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,
            ngram_range=(1, 2),
            min_df=1
        )
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–µ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å—Ö–æ–∂–µ—Å—Ç–∏
        self.similarity_weights = {
            'name': 0.25,
            'formula': 0.30,
            'mass': 0.15,
            'structure': 0.20,
            'properties': 0.10
        }
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        self.thresholds = {
            'mass_tolerance_percent': 0.1,  # 10% –æ—Ç –º–∞—Å—Å—ã
            'min_text_similarity': 0.1,
            'min_structural_similarity': 0.3
        }

    @timing_decorator
    def _clean_smiles_data(self, compound: Dict[str, Any]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö SMILES"""
        cleaned_compound = compound.copy()
        
        smiles = cleaned_compound.get('smiles', '')
        if not self._is_valid_smiles(smiles):
            cleaned_compound['smiles'] = None
            logger.debug(f"–û—á–∏—â–µ–Ω –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π SMILES –¥–ª—è {cleaned_compound.get('name', 'Unknown')}")
        else:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è SMILES
            cleaned_compound['smiles'] = self._normalize_smiles(smiles)
        
        return cleaned_compound

    def _normalize_smiles(self, smiles: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è SMILES —Å—Ç—Ä–æ–∫–∏"""
        if not RDKIT_AVAILABLE or not smiles:
            return smiles
        
        try:
            mol = Chem.MolFromSmiles(smiles)
            if mol:
                return Chem.MolToSmiles(mol, canonical=True)
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ SMILES: {e}")
        
        return smiles

    @timing_decorator
    def find_similar_compounds(self, target_compound: Dict[str, Any],
                              database_type: str, limit: int = 10,
                              compounds_list: Optional[List[Dict[str, Any]]] = None,
                              use_parallel: bool = True) -> List[Tuple[Dict[str, Any], float]]:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            if compounds_list is not None:
                compounds_data = compounds_list
            else:
                compounds_data = self._load_compounds_from_db(database_type)
                
            if not compounds_data:
                return []

            # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            target_compound = self._clean_smiles_data(target_compound)
            compounds_data = [self._clean_smiles_data(comp) for comp in compounds_data]

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏
            if use_parallel and len(compounds_data) > 50:
                similarities = self._calculate_similarities_parallel(
                    target_compound, compounds_data, database_type
                )
            else:
                similarities = self._calculate_similarities_sequential(
                    target_compound, compounds_data, database_type
                )

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            similarities = [(comp, score) for comp, score in similarities if score > 0]
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            return similarities[:limit]

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π: {e}")
            return []

    def _load_compounds_from_db(self, database_type: str) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key_str = f"compounds_{database_type}"
        cached_data = self.compound_cache.get(cache_key_str)
        
        if cached_data:
            return cached_data

        if database_type not in DATABASE_PATHS:
            return []

        db_path = DATABASE_PATHS[database_type]
        if not os.path.exists(db_path):
            return []

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã
            cursor.execute(f"PRAGMA table_info({database_type})")
            columns = [row[1] for row in cursor.fetchall()]

            if not columns:
                return []

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            cursor.execute(f"SELECT * FROM {database_type}")
            all_compounds = cursor.fetchall()
            conn.close()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä–∏
            compounds_data = [dict(zip(columns, row)) for row in all_compounds]
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.compound_cache.set(cache_key_str, compounds_data)
            
            return compounds_data

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ –ë–î {database_type}: {e}")
            return []

    def _calculate_similarities_parallel(self, target_compound: Dict[str, Any],
                                       compounds_data: List[Dict[str, Any]],
                                       database_type: str) -> List[Tuple[Dict[str, Any], float]]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        similarities = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        batch_size = max(10, len(compounds_data) // 4)
        batches = [compounds_data[i:i + batch_size] 
                  for i in range(0, len(compounds_data), batch_size)]
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_batch = {
                executor.submit(
                    self._calculate_batch_similarities,
                    target_compound, batch, database_type
                ): batch for batch in batches
            }
            
            for future in as_completed(future_to_batch):
                try:
                    batch_similarities = future.result()
                    similarities.extend(batch_similarities)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ: {e}")
        
        return similarities

    def _calculate_batch_similarities(self, target_compound: Dict[str, Any],
                                    batch: List[Dict[str, Any]],
                                    database_type: str) -> List[Tuple[Dict[str, Any], float]]:
        """–†–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –±–∞—Ç—á–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        similarities = []
        
        for compound in batch:
            if compound.get('id') == target_compound.get('id'):
                continue
                
            similarity_score = self._calculate_similarity(target_compound, compound, database_type)
            similarities.append((compound, similarity_score))
        
        return similarities

    def _calculate_similarities_sequential(self, target_compound: Dict[str, Any],
                                         compounds_data: List[Dict[str, Any]],
                                         database_type: str) -> List[Tuple[Dict[str, Any], float]]:
        """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        similarities = []
        
        for compound in compounds_data:
            if compound.get('id') == target_compound.get('id'):
                continue
                
            similarity_score = self._calculate_similarity(target_compound, compound, database_type)
            similarities.append((compound, similarity_score))
        
        return similarities

    @lru_cache(maxsize=1000)
    def _calculate_similarity(self, target_compound: Dict[str, Any],
                            compound: Dict[str, Any], database_type: str) -> float:
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞
        cache_key_str = cache_key(
            target_compound.get('id'), 
            compound.get('id'), 
            database_type
        )
        
        cached_similarity = self.similarity_cache.get(cache_key_str)
        if cached_similarity is not None:
            return cached_similarity

        similarity = 0.0
        weights = self.similarity_weights

        # 1. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        name_sim = self._text_similarity(
            target_compound.get('name', ''),
            compound.get('name', '')
        )
        similarity += name_sim * weights['name']

        # 2. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Ñ–æ—Ä–º—É–ª–µ
        if database_type in ['metabolites', 'carbohydrates', 'lipids']:
            formula_sim = self._formula_similarity(
                target_compound.get('formula', ''),
                compound.get('formula', '')
            )
            similarity += formula_sim * weights['formula']

        # 3. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –º–∞—Å—Å–µ
        mass_sim = self._mass_similarity(
            target_compound.get('exact_mass'),
            compound.get('exact_mass')
        )
        similarity += mass_sim * weights['mass']

        # 4. –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        if RDKIT_AVAILABLE:
            target_smiles = target_compound.get('smiles')
            comp_smiles = compound.get('smiles')
            
            if target_smiles and comp_smiles and \
               self._is_valid_smiles(target_smiles) and self._is_valid_smiles(comp_smiles):
                struct_sim = self._structural_similarity(target_smiles, comp_smiles)
                similarity += struct_sim * weights['structure']

        # 5. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å–≤–æ–π—Å—Ç–≤–∞–º
        if database_type == 'enzymes':
            prop_sim = self._enzyme_similarity(target_compound, compound)
        elif database_type == 'proteins':
            prop_sim = self._protein_similarity(target_compound, compound)
        else:
            prop_sim = 0.0
            
        similarity += prop_sim * weights['properties']

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final_similarity = min(similarity, 1.0)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.similarity_cache.set(cache_key_str, final_similarity)
        
        return final_similarity

    def _text_similarity(self, text1: str, text2: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not text1 or not text2:
            return 0.0

        text1 = text1.lower().strip()
        text2 = text2.lower().strip()
        
        if text1 == text2:
            return 1.0

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫
        # 1. Jaccard similarity –¥–ª—è —Å–ª–æ–≤
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
            
        jaccard_sim = len(words1 & words2) / len(words1 | words2)
        
        # 2. –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ–¥—Å—Ç—Ä–æ–∫
        substring_sim = 0.0
        if len(text1) > 2 and len(text2) > 2:
            # –ò—â–µ–º –æ–±—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –¥–ª–∏–Ω–æ–π 3+
            substrings1 = {text1[i:i+3] for i in range(len(text1)-2)}
            substrings2 = {text2[i:i+3] for i in range(len(text2)-2)}
            if substrings1 and substrings2:
                substring_sim = len(substrings1 & substrings2) / len(substrings1 | substrings2)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
        return 0.7 * jaccard_sim + 0.3 * substring_sim

    def _formula_similarity(self, formula1: str, formula2: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ö–∏–º–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª"""
        if not formula1 or not formula2:
            return 0.0

        elements1 = self._parse_formula(formula1)
        elements2 = self._parse_formula(formula2)

        if not elements1 or not elements2:
            return 0.0

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º
        all_elements = set(elements1.keys()) | set(elements2.keys())
        
        if not all_elements:
            return 0.0

        # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª
        vector1 = [elements1.get(elem, 0) for elem in all_elements]
        vector2 = [elements2.get(elem, 0) for elem in all_elements]
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        dot_product = sum(a * b for a, b in zip(vector1, vector2))
        magnitude1 = math.sqrt(sum(a * a for a in vector1))
        magnitude2 = math.sqrt(sum(b * b for b in vector2))
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
            
        cosine_sim = dot_product / (magnitude1 * magnitude2)
        
        # Jaccard similarity –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        set1 = set(elements1.keys())
        set2 = set(elements2.keys())
        jaccard_sim = len(set1 & set2) / len(set1 | set2) if (set1 | set2) else 0.0
        
        return 0.6 * cosine_sim + 0.4 * jaccard_sim

    def _parse_formula(self, formula: str) -> Dict[str, int]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ö–∏–º–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—É–ª—ã"""
        if not formula:
            return {}
            
        elements = {}
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        pattern = r'([A-Z][a-z]?)(\d*)'
        matches = re.findall(pattern, formula)

        for element, count in matches:
            count = int(count) if count else 1
            elements[element] = elements.get(element, 0) + count

        return elements

    def _mass_similarity(self, mass1: Optional[float], mass2: Optional[float]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ –º–æ–ª–µ–∫—É–ª—è—Ä–Ω–æ–π –º–∞—Å—Å–µ"""
        if mass1 is None or mass2 is None:
            return 0.0

        mass1, mass2 = float(mass1), float(mass2)
        
        if mass1 == mass2:
            return 1.0

        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–ª–µ–∫—É–ª—ã
        avg_mass = (mass1 + mass2) / 2
        tolerance = avg_mass * self.thresholds['mass_tolerance_percent']
        
        diff = abs(mass1 - mass2)
        
        if diff <= tolerance:
            # –õ–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            return 1.0 - (diff / tolerance)
        else:
            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            sigma = tolerance * 2
            return math.exp(-(diff ** 2) / (2 * sigma ** 2))

    @lru_cache(maxsize=500)
    def _structural_similarity(self, smiles1: str, smiles2: str) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if not RDKIT_AVAILABLE:
            return 0.0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à fingerprints
        fp_cache_key1 = f"fp_{smiles1}"
        fp_cache_key2 = f"fp_{smiles2}"
        
        fp1 = self.fingerprint_cache.get(fp_cache_key1)
        fp2 = self.fingerprint_cache.get(fp_cache_key2)

        try:
            if fp1 is None:
                mol1 = Chem.MolFromSmiles(smiles1)
                if mol1 is None:
                    return 0.0
                fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, nBits=2048)
                self.fingerprint_cache.set(fp_cache_key1, fp1)

            if fp2 is None:
                mol2 = Chem.MolFromSmiles(smiles2)
                if mol2 is None:
                    return 0.0
                fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, nBits=2048)
                self.fingerprint_cache.set(fp_cache_key2, fp2)

            # Tanimoto similarity
            similarity = DataStructs.TanimotoSimilarity(fp1, fp2)
            return similarity

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
            return 0.0

    def _is_valid_smiles(self, smiles: str) -> bool:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ SMILES"""
        if not smiles or not isinstance(smiles, str):
            return False
        
        smiles = smiles.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        invalid_values = {'0', 'None', 'null', '', 'nan', 'NaN', 'NULL', 'n/a', 'N/A'}
        if smiles in invalid_values:
            return False
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(smiles) < 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ö–∏–º–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
        valid_chars = set('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789()[]{}@+-=#$%:;,.')
        if not all(char in valid_chars for char in smiles):
            return False
        
        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω RDKit, –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –Ω–µ–≥–æ
        if RDKIT_AVAILABLE:
            try:
                mol = Chem.MolFromSmiles(smiles)
                return mol is not None
            except:
                return False
        
        return True

    def _enzyme_similarity(self, enzyme1: Dict[str, Any], enzyme2: Dict[str, Any]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤"""
        similarity = 0.0

        # EC –Ω–æ–º–µ—Ä (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ)
        ec1 = enzyme1.get('ec_number', '')
        ec2 = enzyme2.get('ec_number', '')

        if ec1 and ec2:
            ec_parts1 = ec1.split('.')
            ec_parts2 = ec2.split('.')
            
            # –í–µ—Å—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π EC –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            level_weights = [0.4, 0.3, 0.2, 0.1]
            ec_similarity = 0.0
            
            for i in range(min(len(ec_parts1), len(ec_parts2), 4)):
                if ec_parts1[i] == ec_parts2[i]:
                    ec_similarity += level_weights[i]
                else:
                    break
            
            similarity += ec_similarity * 0.6

        # –°–µ–º–µ–π—Å—Ç–≤–æ
        family1 = enzyme1.get('family', '')
        family2 = enzyme2.get('family', '')
        if family1 and family2:
            family_sim = 1.0 if family1.lower() == family2.lower() else 0.0
            similarity += family_sim * 0.3

        # –§—É–Ω–∫—Ü–∏—è (—Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
        func1 = enzyme1.get('function', '')
        func2 = enzyme2.get('function', '')
        if func1 and func2:
            func_sim = self._text_similarity(func1, func2)
            similarity += func_sim * 0.1

        return similarity

    def _protein_similarity(self, protein1: Dict[str, Any], protein2: Dict[str, Any]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –±–µ–ª–∫–æ–≤"""
        similarity = 0.0

        # –§—É–Ω–∫—Ü–∏—è
        func1 = protein1.get('function', '')
        func2 = protein2.get('function', '')
        if func1 and func2:
            func_sim = self._text_similarity(func1, func2)
            similarity += func_sim * 0.4

        # –°–µ–º–µ–π—Å—Ç–≤–æ
        family1 = protein1.get('family', '')
        family2 = protein2.get('family', '')
        if family1 and family2:
            family_sim = 1.0 if family1.lower() == family2.lower() else 0.0
            similarity += family_sim * 0.3

        # –û—Ä–≥–∞–Ω–∏–∑–º
        org1 = protein1.get('organism', '')
        org2 = protein2.get('organism', '')
        if org1 and org2:
            org_sim = 1.0 if org1.lower() == org2.lower() else 0.0
            similarity += org_sim * 0.2

        # –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
        len1 = protein1.get('sequence_length')
        len2 = protein2.get('sequence_length')
        if len1 and len2:
            len_sim = self._mass_similarity(float(len1), float(len2))  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É
            similarity += len_sim * 0.1

        return similarity

    def _apply_filters(self, compounds: List[Dict[str, Any]], 
                      mass_range: Tuple[float, float],
                      smiles_only: bool, 
                      keyword_filter: str,
                      formula_elements: List[str]) -> List[Dict[str, Any]]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        initial_count = len(compounds)
        filtered_compounds = []

        for compound in compounds:
            # –§–∏–ª—å—Ç—Ä –ø–æ –º–∞—Å—Å–µ
            mass = compound.get('exact_mass')
            if mass is not None:
                if mass < mass_range[0] or mass > mass_range[1]:
                    continue

            # –§–∏–ª—å—Ç—Ä –ø–æ SMILES
            if smiles_only:
                if not self._is_valid_smiles(compound.get('smiles', '')):
                    continue

            # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if keyword_filter:
                keywords = [kw.strip().lower() for kw in keyword_filter.split(',') if kw.strip()]
                name = compound.get('name', '').lower()
                if not any(keyword in name for keyword in keywords):
                    continue

            # –§–∏–ª—å—Ç—Ä –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º –≤ —Ñ–æ—Ä–º—É–ª–µ
            if formula_elements:
                formula = compound.get('formula', '')
                if formula:
                    elements_in_formula = set(self._parse_formula(formula).keys())
                    if not all(elem in elements_in_formula for elem in formula_elements):
                        continue

            filtered_compounds.append(compound)

        logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {initial_count} ‚Üí {len(filtered_compounds)} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")
        return filtered_compounds

    @timing_decorator
    def cluster_compounds(self, compounds: List[Dict[str, Any]],
                         database_type: str, n_clusters: int = 5) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        if not compounds or len(compounds) < n_clusters:
            return {"error": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            features = self._extract_features(compounds, database_type)
            
            if not features or len(features[0]) == 0:
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_array = np.array(features)
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features_array)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            optimal_clusters = min(n_clusters, len(compounds) // 2)
            
            # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            kmeans = KMeans(
                n_clusters=optimal_clusters, 
                random_state=42, 
                n_init=10,
                max_iter=300
            )
            clusters = kmeans.fit_predict(features_scaled)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            cluster_results = defaultdict(list)
            for i, cluster_id in enumerate(clusters):
                cluster_results[cluster_id].append(compounds[i])

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            cluster_stats = {}
            for cluster_id, cluster_compounds in cluster_results.items():
                cluster_stats[cluster_id] = {
                    'size': len(cluster_compounds),
                    'avg_mass': np.mean([c.get('exact_mass', 0) for c in cluster_compounds if c.get('exact_mass')]),
                    'common_elements': self._find_common_elements(cluster_compounds)
                }

            return {
                "clusters": dict(cluster_results),
                "n_clusters": optimal_clusters,
                "total_compounds": len(compounds),
                "cluster_stats": cluster_stats,
                "silhouette_score": self._calculate_silhouette_score(features_scaled, clusters)
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return {"error": str(e)}

    def _extract_features(self, compounds: List[Dict[str, Any]], 
                         database_type: str) -> List[List[float]]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        features = []

        for compound in compounds:
            feature_vector = []

            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            mass = compound.get('exact_mass', 0)
            feature_vector.append(float(mass) if mass else 0.0)
            
            name_len = len(compound.get('name', ''))
            feature_vector.append(float(name_len))
            
            formula_len = len(compound.get('formula', ''))
            feature_vector.append(float(formula_len))

            # –ü—Ä–∏–∑–Ω–∞–∫–∏ —Ñ–æ—Ä–º—É–ª—ã
            formula = compound.get('formula', '')
            if formula:
                elements = self._parse_formula(formula)
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                feature_vector.append(float(len(elements)))
                # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—Ç–æ–º–æ–≤
                feature_vector.append(float(sum(elements.values())))
                # –û—Ç–Ω–æ—à–µ–Ω–∏–µ C –∫ –¥—Ä—É–≥–∏–º —ç–ª–µ–º–µ–Ω—Ç–∞–º
                c_count = elements.get('C', 0)
                total_atoms = sum(elements.values())
                c_ratio = c_count / total_atoms if total_atoms > 0 else 0
                feature_vector.append(c_ratio)
            else:
                feature_vector.extend([0.0, 0.0, 0.0])

            # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ —Ç–∏–ø—É –±–∞–∑—ã
            if database_type == 'enzymes':
                ec_number = compound.get('ec_number', '0.0.0.0')
                ec_parts = ec_number.split('.')
                for i in range(4):
                    if i < len(ec_parts):
                        try:
                            feature_vector.append(float(ec_parts[i]))
                        except:
                            feature_vector.append(0.0)
                    else:
                        feature_vector.append(0.0)
                        
            elif database_type == 'proteins':
                seq_len = compound.get('sequence_length', 0)
                feature_vector.append(float(seq_len) if seq_len else 0.0)
                
                func_len = len(compound.get('function', ''))
                feature_vector.append(float(func_len))
                
                family_len = len(compound.get('family', ''))
                feature_vector.append(float(family_len))
                
            else:
                # –î–ª—è –º–µ—Ç–∞–±–æ–ª–∏—Ç–æ–≤, —É–≥–ª–µ–≤–æ–¥–æ–≤, –ª–∏–ø–∏–¥–æ–≤
                class_len = len(compound.get('class_name', ''))
                feature_vector.append(float(class_len))
                
                # SMILES –ø—Ä–∏–∑–Ω–∞–∫–∏
                smiles = compound.get('smiles', '')
                if smiles and self._is_valid_smiles(smiles):
                    feature_vector.append(float(len(smiles)))
                    feature_vector.append(float(smiles.count('=')))  # –î–≤–æ–π–Ω—ã–µ —Å–≤—è–∑–∏
                    feature_vector.append(float(smiles.count('#')))  # –¢—Ä–æ–π–Ω—ã–µ —Å–≤—è–∑–∏
                    feature_vector.append(float(smiles.count('(')))  # –í–µ—Ç–≤–ª–µ–Ω–∏—è
                else:
                    feature_vector.extend([0.0, 0.0, 0.0, 0.0])

            features.append(feature_vector)

        return features

    def _find_common_elements(self, compounds: List[Dict[str, Any]]) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ —Ö–∏–º–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ"""
        if not compounds:
            return []
            
        element_sets = []
        for compound in compounds:
            formula = compound.get('formula', '')
            if formula:
                elements = set(self._parse_formula(formula).keys())
                element_sets.append(elements)
        
        if not element_sets:
            return []
            
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–Ω–æ–∂–µ—Å—Ç–≤
        common_elements = element_sets[0]
        for elem_set in element_sets[1:]:
            common_elements &= elem_set
            
        return list(common_elements)

    def _calculate_silhouette_score(self, features: np.ndarray, clusters: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∏–ª—É—ç—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
        try:
            from sklearn.metrics import silhouette_score
            if len(set(clusters)) > 1:
                return silhouette_score(features, clusters)
        except:
            pass
        return 0.0

    def get_recommendation_explanation(self, target_compound: Dict[str, Any],
                                     similar_compound: Dict[str, Any],
                                     similarity_score: float) -> str:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        explanations = []

        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        name1 = target_compound.get('name', '').lower()
        name2 = similar_compound.get('name', '').lower()
        
        if name1 and name2:
            common_words = set(name1.split()) & set(name2.split())
            if common_words:
                explanations.append(f"–û–±—â–∏–µ —Å–ª–æ–≤–∞: {', '.join(sorted(common_words))}")

        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ —Ñ–æ—Ä–º—É–ª–µ
        formula1 = target_compound.get('formula', '')
        formula2 = similar_compound.get('formula', '')
        
        if formula1 and formula2:
            elements1 = set(self._parse_formula(formula1).keys())
            elements2 = set(self._parse_formula(formula2).keys())
            common_elements = elements1 & elements2
            
            if common_elements:
                explanations.append(f"–û–±—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {', '.join(sorted(common_elements))}")

        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ –º–∞—Å—Å–µ
        mass1 = target_compound.get('exact_mass')
        mass2 = similar_compound.get('exact_mass')
        
        if mass1 and mass2:
            mass_diff = abs(mass1 - mass2)
            mass_diff_percent = (mass_diff / max(mass1, mass2)) * 100
            
            if mass_diff_percent < 5:
                explanations.append(f"–û—á–µ–Ω—å –±–ª–∏–∑–∫–∏–µ –º–∞—Å—Å—ã (—Ä–∞–∑–Ω–∏—Ü–∞ {mass_diff_percent:.1f}%)")
            elif mass_diff_percent < 15:
                explanations.append(f"–ë–ª–∏–∑–∫–∏–µ –º–∞—Å—Å—ã (—Ä–∞–∑–Ω–∏—Ü–∞ {mass_diff_percent:.1f}%)")

        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        if RDKIT_AVAILABLE and similarity_score > 0.6:
            smiles1 = target_compound.get('smiles')
            smiles2 = similar_compound.get('smiles')
            if smiles1 and smiles2:
                struct_sim = self._structural_similarity(smiles1, smiles2)
                if struct_sim > 0.7:
                    explanations.append(f"–í—ã—Å–æ–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å ({struct_sim:.1%})")

        if not explanations:
            explanations.append(f"–û–±—â–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏: {similarity_score:.1%}")

        return " | ".join(explanations)


def render_recommendations_interface():
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    st.header("üéØ –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–∏–∂–∫–∞
    if 'recommendation_engine' not in st.session_state:
        st.session_state.recommendation_engine = RecommendationsEngine()
    
    engine = st.session_state.recommendation_engine

    # –í—ã–±–æ—Ä —Ç–∏–ø–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    database_options = {
        "metabolites": "üß¨ –ú–µ—Ç–∞–±–æ–ª–∏—Ç—ã",
        "enzymes": "üß™ –§–µ—Ä–º–µ–Ω—Ç—ã", 
        "proteins": "üî¨ –ë–µ–ª–∫–∏",
        "carbohydrates": "üåæ –£–≥–ª–µ–≤–æ–¥—ã",
        "lipids": "ü´ß –õ–∏–ø–∏–¥—ã"
    }

    selected_db = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:",
        options=list(database_options.keys()),
        format_func=lambda x: database_options[x]
    )

    if selected_db:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ..."):
            compounds_list = engine._load_compounds_from_db(selected_db)

        if not compounds_list:
            st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã {database_options[selected_db]}")
            return

        st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(compounds_list):,} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏–∑ –±–∞–∑—ã {database_options[selected_db]}")

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –¥–∞–Ω–Ω—ã–µ..."):
            compounds_list = [engine._clean_smiles_data(comp) for comp in compounds_list]

        # –í—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ø–æ–∏—Å–∫–æ–º
        st.subheader("üéØ –í—ã–±–æ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
        
        # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        search_query = st.text_input(
            "–ü–æ–∏—Å–∫ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é:",
            placeholder="–í–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞..."
        )
        
        if search_query:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –ø–æ–∏—Å–∫–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É
            filtered_compounds = [
                comp for comp in compounds_list 
                if search_query.lower() in comp.get('name', '').lower()
            ]
            if filtered_compounds:
                st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_compounds)} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_query}'")
                display_compounds = filtered_compounds[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            else:
                st.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ –∑–∞–ø—Ä–æ—Å—É '{search_query}'")
                display_compounds = compounds_list[:100]
        else:
            display_compounds = compounds_list[:100]

        # –í—ã–±–æ—Ä —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        compound_names = [
            f"{c.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]} (ID: {c.get('id', '‚Äî')}, "
            f"–ú–∞—Å—Å–∞: {c.get('exact_mass', '‚Äî')})" 
            for c in display_compounds
        ]
        
        selected_compound_idx = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤:",
            options=range(len(display_compounds)),
            format_func=lambda x: compound_names[x]
        )

        target_compound = display_compounds[selected_compound_idx]

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–∏
        with st.expander("‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–∏", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                st.write(f"**–ù–∞–∑–≤–∞–Ω–∏–µ:** {target_compound.get('name', '‚Äî')}")
                st.write(f"**–§–æ—Ä–º—É–ª–∞:** {target_compound.get('formula', '‚Äî')}")
                st.write(f"**–ú–∞—Å—Å–∞:** {target_compound.get('exact_mass', '‚Äî')} Da")
            with col2:
                st.write(f"**ID:** {target_compound.get('id', '‚Äî')}")
                st.write(f"**SMILES:** {target_compound.get('smiles', '‚Äî')}")
                if target_compound.get('class_name'):
                    st.write(f"**–ö–ª–∞—Å—Å:** {target_compound.get('class_name')}")

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        st.subheader("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞")

        col1, col2 = st.columns(2)
        with col1:
            limit = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:", 5, 100, 15)
            use_parallel = st.checkbox("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", value=True, 
                                     help="–£—Å–∫–æ—Ä—è–µ—Ç –ø–æ–∏—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö")
        with col2:
            min_similarity = st.slider("–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å (%):", 0, 100, 20) / 100.0
            show_explanations = st.checkbox("–ü–æ–∫–∞–∑–∞—Ç—å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è", value=True)

        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        with st.expander("üîç –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã"):
            col3, col4 = st.columns(2)

            with col3:
                # –§–∏–ª—å—Ç—Ä –ø–æ –º–∞—Å—Å–µ
                mass_range = st.slider(
                    "–î–∏–∞–ø–∞–∑–æ–Ω –º–∞—Å—Å—ã (Da):",
                    0.0, 2000.0, (0.0, 2000.0),
                    help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–æ–∏—Å–∫ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è–º–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –º–∞—Å—Å"
                )

                # –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–ª–∏—á–∏—é SMILES
                smiles_only = st.checkbox(
                    "–¢–æ–ª—å–∫–æ —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ SMILES",
                    help="–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ SMILES"
                )

            with col4:
                # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                keyword_filter = st.text_input(
                    "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏:",
                    placeholder="glucose, acid, dehydrogenase",
                    help="–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ —Å–ª–æ–≤–∞–º –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)"
                )

                # –§–∏–ª—å—Ç—Ä –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º –≤ —Ñ–æ—Ä–º—É–ª–µ
                formula_elements = st.multiselect(
                    "–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:",
                    options=["C", "H", "O", "N", "P", "S", "Cl", "Br", "I", "F", "Na", "K", "Ca", "Mg"],
                    help="–°–æ–µ–¥–∏–Ω–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"
                )

        # –ü–æ–∏—Å–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        if st.button("üîç –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è", type="primary"):
            with st.spinner("–ò—â—É –ø–æ—Ö–æ–∂–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è..."):
                start_time = time.time()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                filtered_compounds = engine._apply_filters(
                    compounds_list, mass_range, smiles_only, 
                    keyword_filter, formula_elements
                )

                if len(filtered_compounds) < 2:
                    st.warning("‚ö†Ô∏è –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")
                    return

                # –ü–æ–∏—Å–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
                similar_compounds = engine.find_similar_compounds(
                    target_compound, selected_db, limit, 
                    filtered_compounds, use_parallel
                )

                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
                final_results = [
                    (comp, score) for comp, score in similar_compounds 
                    if score >= min_similarity
                ]

                search_time = time.time() - start_time
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                st.session_state.recommendation_results = final_results
                st.session_state.target_compound = target_compound
                st.session_state.search_params = {
                    'database': selected_db,
                    'limit': limit,
                    'min_similarity': min_similarity,
                    'search_time': search_time,
                    'show_explanations': show_explanations
                }

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if hasattr(st.session_state, 'recommendation_results') and st.session_state.recommendation_results:
            results = st.session_state.recommendation_results
            target = st.session_state.target_compound
            params = st.session_state.search_params

            st.success(f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ—Ö–æ–∂–∏—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∑–∞ {params['search_time']:.2f}—Å")
            
            st.subheader(f"üìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è: {target.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

            if results:
                # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                result_data = []
                for i, (comp, similarity) in enumerate(results):
                    row_data = {
                        "‚Ññ": i + 1,
                        "–ù–∞–∑–≤–∞–Ω–∏–µ": comp.get('name', '‚Äî'),
                        "–°—Ö–æ–∂–µ—Å—Ç—å": f"{similarity:.1%}",
                        "–§–æ—Ä–º—É–ª–∞": comp.get('formula', '‚Äî'),
                        "–ú–∞—Å—Å–∞ (Da)": f"{comp.get('exact_mass', 0):.2f}" if comp.get('exact_mass') else "‚Äî",
                    }
                    
                    if params.get('show_explanations', True):
                        row_data["–û–±—ä—è—Å–Ω–µ–Ω–∏–µ"] = engine.get_recommendation_explanation(
                            target, comp, similarity
                        )
                    
                    result_data.append(row_data)

                df = pd.DataFrame(result_data)
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã
                column_config = {
                    "‚Ññ": st.column_config.NumberColumn(width="small"),
                    "–ù–∞–∑–≤–∞–Ω–∏–µ": st.column_config.TextColumn(width="large"),
                    "–°—Ö–æ–∂–µ—Å—Ç—å": st.column_config.TextColumn(width="small"),
                    "–§–æ—Ä–º—É–ª–∞": st.column_config.TextColumn(width="medium"),
                    "–ú–∞—Å—Å–∞ (Da)": st.column_config.NumberColumn(width="small", format="%.2f"),
                }
                
                if params.get('show_explanations', True):
                    column_config["–û–±—ä—è—Å–Ω–µ–Ω–∏–µ"] = st.column_config.TextColumn(width="large")

                st.dataframe(
                    df,
                    use_container_width=True,
                    hide_index=True,
                    column_config=column_config
                )

                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
                st.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                similarities = [sim for _, sim in results]
                names = [comp.get('name', '‚Äî')[:30] + '...' if len(comp.get('name', '')) > 30 
                        else comp.get('name', '‚Äî') for comp, _ in results]

                # –ì—Ä–∞—Ñ–∏–∫ —Å—Ö–æ–∂–µ—Å—Ç–∏
                fig = go.Figure()
                
                # –¶–≤–µ—Ç–æ–≤–∞—è —Å—Ö–µ–º–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
                colors = ['#2E8B57' if s >= 0.7 else '#4682B4' if s >= 0.5 else '#CD853F' 
                         for s in similarities]
                
                fig.add_trace(go.Bar(
                    x=names,
                    y=[s * 100 for s in similarities],
                    marker_color=colors,
                    text=[f'{s*100:.1f}%' for s in similarities],
                    textposition='auto',
                    hovertemplate='<b>%{x}</b><br>–°—Ö–æ–∂–µ—Å—Ç—å: %{y:.1f}%<extra></extra>'
                ))

                fig.update_layout(
                    title=f"–°—Ö–æ–∂–µ—Å—Ç—å —Å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º: {target.get('name', '‚Äî')}",
                    xaxis_title="–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ",
                    yaxis_title="–°—Ö–æ–∂–µ—Å—Ç—å (%)",
                    height=500,
                    xaxis_tickangle=-45,
                    showlegend=False
                )

                st.plotly_chart(fig, use_container_width=True)

                # –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                col1, col2 = st.columns(2)
                with col1:
                    csv_data = df.to_csv(index=False, encoding='utf-8')
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (CSV)",
                        data=csv_data,
                        file_name=f"recommendations_{selected_db}_{target.get('name', 'compound')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                
                with col2:
                    # JSON —ç–∫—Å–ø–æ—Ä—Ç —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    json_data = {
                        'target_compound': target,
                        'recommendations': [
                            {'compound': comp, 'similarity': float(sim)} 
                            for comp, sim in results
                        ],
                        'search_parameters': params
                    }
                    import json
                    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
                    st.download_button(
                        label="üì• –°–∫–∞—á–∞—Ç—å –¥–∞–Ω–Ω—ã–µ (JSON)",
                        data=json_str,
                        file_name=f"recommendations_{selected_db}_{target.get('name', 'compound')}.json",
                        mime="application/json",
                        use_container_width=True
                    )

            else:
                st.info("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ–∏—Å–∫–∞")

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        st.divider()
        st.subheader("üìä –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

        col1, col2 = st.columns(2)
        with col1:
            n_clusters = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:", 2, 15, 5)
        with col2:
            cluster_sample_size = st.slider("–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:", 100, min(2000, len(compounds_list)), 500)

        if st.button("üéØ –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é"):
            with st.spinner("–í—ã–ø–æ–ª–Ω—è—é –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑..."):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                sample_compounds = compounds_list[:cluster_sample_size]
                cluster_results = engine.cluster_compounds(sample_compounds, selected_db, n_clusters)

                if "error" not in cluster_results:
                    st.success(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {cluster_results['n_clusters']} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ {cluster_results['total_compounds']} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π")
                    
                    if 'silhouette_score' in cluster_results and cluster_results['silhouette_score'] > 0:
                        st.info(f"üìä –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∏–ª—É—ç—Ç–∞: {cluster_results['silhouette_score']:.3f} (–∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏)")

                    clusters = cluster_results['clusters']
                    cluster_stats = cluster_results.get('cluster_stats', {})

                    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    st.subheader("üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                    
                    # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    cluster_sizes = [len(compounds) for compounds in clusters.values()]
                    cluster_labels = [f"–ö–ª–∞—Å—Ç–µ—Ä {i+1}" for i in range(len(clusters))]
                    
                    fig_pie = go.Figure(data=[go.Pie(
                        labels=cluster_labels,
                        values=cluster_sizes,
                        hole=0.3
                    )])
                    fig_pie.update_layout(title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
                    st.plotly_chart(fig_pie, use_container_width=True)

                    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    for cluster_id, cluster_compounds in clusters.items():
                        stats = cluster_stats.get(cluster_id, {})
                        
                        with st.expander(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1} ({len(cluster_compounds)} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π)"):
                            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
                            if stats:
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("–†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞", len(cluster_compounds))
                                with col2:
                                    avg_mass = stats.get('avg_mass', 0)
                                    st.metric("–°—Ä–µ–¥–Ω—è—è –º–∞—Å—Å–∞", f"{avg_mass:.1f} Da" if avg_mass else "‚Äî")
                                with col3:
                                    common_elems = stats.get('common_elements', [])
                                    st.metric("–û–±—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã", f"{len(common_elems)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                                    if common_elems:
                                        st.write(f"–≠–ª–µ–º–µ–Ω—Ç—ã: {', '.join(common_elems)}")

                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–∞
                            cluster_data = []
                            for comp in cluster_compounds[:10]:
                                cluster_data.append({
                                    "–ù–∞–∑–≤–∞–Ω–∏–µ": comp.get('name', '‚Äî'),
                                    "–§–æ—Ä–º—É–ª–∞": comp.get('formula', '‚Äî'),
                                    "–ú–∞—Å—Å–∞": f"{comp.get('exact_mass', 0):.2f}" if comp.get('exact_mass') else "‚Äî"
                                })

                            if cluster_data:
                                cluster_df = pd.DataFrame(cluster_data)
                                st.dataframe(cluster_df, use_container_width=True, hide_index=True)
                                
                                if len(cluster_compounds) > 10:
                                    st.info(f"–ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 10 –∏–∑ {len(cluster_compounds)} —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∫–ª–∞—Å—Ç–µ—Ä–∞")

                    # –≠–∫—Å–ø–æ—Ä—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    st.subheader("üíæ –≠–∫—Å–ø–æ—Ä—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                    
                    export_data = []
                    for cluster_id, cluster_compounds in clusters.items():
                        for comp in cluster_compounds:
                            export_data.append({
                                "–ö–ª–∞—Å—Ç–µ—Ä": f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id + 1}",
                                "–ù–∞–∑–≤–∞–Ω–∏–µ": comp.get('name', '‚Äî'),
                                "–§–æ—Ä–º—É–ª–∞": comp.get('formula', '‚Äî'),
                                "–ú–∞—Å—Å–∞": comp.get('exact_mass', '‚Äî'),
                                "–¢–∏–ø": database_options[selected_db]
                            })

                    if export_data:
                        export_df = pd.DataFrame(export_data)
                        csv_cluster_data = export_df.to_csv(index=False, encoding='utf-8')
                        st.download_button(
                            label="üì• –°–∫–∞—á–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã (CSV)",
                            data=csv_cluster_data,
                            file_name=f"clusters_{selected_db}_{n_clusters}_clusters.csv",
                            mime="text/csv",
                            use_container_width=True
                        )

                else:
                    st.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {cluster_results['error']}")

    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∏—Å—Ç–µ–º–µ
    with st.expander("‚ÑπÔ∏è –û —Å–∏—Å—Ç–µ–º–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"):
        st.markdown("""
        **üéØ –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–¥—Ö–æ–¥:

        ### üîç –ê–ª–≥–æ—Ä–∏—Ç–º—ã —Å—Ö–æ–∂–µ—Å—Ç–∏:
        - **–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ**: Jaccard similarity + –∞–Ω–∞–ª–∏–∑ –ø–æ–¥—Å—Ç—Ä–æ–∫
        - **–•–∏–º–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞**: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–Ω–æ–≥–æ —Å–æ—Å—Ç–∞–≤–∞
        - **–ú–æ–ª–µ–∫—É–ª—è—Ä–Ω–∞—è –º–∞—Å—Å–∞**: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å + –≥–∞—É—Å—Å–æ–≤–æ —Å—Ö–æ–¥—Å—Ç–≤–æ
        - **–°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ**: Morgan fingerprints + Tanimoto similarity (RDKit)
        - **–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞**: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª—è —Ñ–µ—Ä–º–µ–Ω—Ç–æ–≤ –∏ –±–µ–ª–∫–æ–≤

        ### ‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
        - **–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ** —Å TTL
        - **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        - **–£–º–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** SMILES —Å—Ç—Ä—É–∫—Ç—É—Ä
        - **–ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** —Å—Ö–æ–∂–µ—Å—Ç–∏

        ### üìä –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
        - **K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è** —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        - **–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∏–ª—É—ç—Ç–∞** –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        - **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤** —Å –æ–±—â–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
        - **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è** —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

        ### üõ†Ô∏è –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:
        - –ù–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤
        - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        - –ì–∏–±–∫–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        - –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV –∏ JSON —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        """)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if hasattr(st.session_state, 'recommendation_engine'):
            engine = st.session_state.recommendation_engine
            st.subheader("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("–ö—ç—à —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π", len(engine.compound_cache.cache))
            with col2:
                st.metric("–ö—ç—à —Å—Ö–æ–∂–µ—Å—Ç–∏", len(engine.similarity_cache.cache))
            with col3:
                st.metric("–ö—ç—à fingerprints", len(engine.fingerprint_cache.cache))


if __name__ == "__main__":
    render_recommendations_interface()